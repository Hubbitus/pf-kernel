diff --git a/arch/x86/kernel/ioport.c b/arch/x86/kernel/ioport.c
index 99c4d30..c8e374f 100644
--- a/arch/x86/kernel/ioport.c
+++ b/arch/x86/kernel/ioport.c
@@ -41,9 +41,28 @@ asmlinkage long sys_ioperm(unsigned long from, unsigned long num, int turn_on)
 
 	if ((from + num <= from) || (from + num > IO_BITMAP_BITS))
 		return -EINVAL;
+#if defined(CONFIG_CPU_BFS_AUTOISO)
+	if (turn_on) {
+		struct sched_param param = { .sched_priority = 0 };
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+		/* Start X as SCHED_ISO */
+		sched_setscheduler_nocheck(current, SCHED_ISO, &param);
+	}
+#elif defined(CONFIG_CFS_BOOST)
+	if (turn_on) {
+		if (!capable(CAP_SYS_RAWIO))
+		return -EPERM;
+		/*
+		 * Task will be accessing hardware IO ports,
+		 * mark it as special with the scheduler too:
+		 */
+	sched_privileged_task(current);
+}
+#else
 	if (turn_on && !capable(CAP_SYS_RAWIO))
 		return -EPERM;
-
+#endif
 	/*
 	 * If it's the first ioperm() call in this thread's lifetime, set the
 	 * IO bitmap up. ioperm() is much less timing critical than clone(),
@@ -111,8 +130,20 @@ static int do_iopl(unsigned int level, struct pt_regs *regs)
 		return -EINVAL;
 	/* Trying to gain more privileges? */
 	if (level > old) {
+#if defined(CONFIG_CPU_BFS_AUTOISO)
+		struct sched_param param = { .sched_priority = 0 };
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+		/* Start X as SCHED_ISO */
+		sched_setscheduler_nocheck(current, SCHED_ISO, &param);
+#elif defined(CONFIG_CFS_BOOST)
 		if (!capable(CAP_SYS_RAWIO))
 			return -EPERM;
+			sched_privileged_task(current);
+#else
+		if (!capable(CAP_SYS_RAWIO))
+			return -EPERM;
+#endif
 	}
 	regs->flags = (regs->flags & ~X86_EFLAGS_IOPL) | (level << 12);
 
diff --git a/drivers/block/loop.c b/drivers/block/loop.c
index bd112c8..21e0448 100644
--- a/drivers/block/loop.c
+++ b/drivers/block/loop.c
@@ -593,7 +593,14 @@ static int loop_thread(void *data)
 	struct loop_device *lo = data;
 	struct bio *bio;
 
+#if defined(CONFIG_CFS_BOOST)
+	/*
+	 * The loop thread is important enough to be given a boost:
+	 */
+	sched_privileged_task(current);
+#else
 	set_user_nice(current, -20);
+#endif
 
 	while (!kthread_should_stop() || !bio_list_empty(&lo->lo_bio_list)) {
 
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 6d4ade5..6dfcb2b 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1993,6 +1993,10 @@ static inline int rt_mutex_getprio(struct task_struct *p)
 #endif
 
 extern void set_user_nice(struct task_struct *p, long nice);
+#ifdef CONFIG_CFS_BOOST
+extern void sched_privileged_task(struct task_struct *p);
+extern int sysctl_sched_privileged_nice_level;
+#endif
 extern int task_prio(const struct task_struct *p);
 extern int task_nice(const struct task_struct *p);
 extern int can_nice(const struct task_struct *p, const int nice);
diff --git a/init/Kconfig.cpu b/init/Kconfig.cpu
index 514c496..5d47254 100644
--- a/init/Kconfig.cpu
+++ b/init/Kconfig.cpu
@@ -22,3 +22,67 @@ config CPU_BFS
 	  Not recommended for 4096 CPUs
 
 endchoice
+
+config CPU_BFS_AUTOISO
+        bool "Auto SCHED_ISO for X"
+        depends on CPU_BFS
+        help
+	  Tasks (including X) can be run as SCHED_ISO manually with schedtool -I
+	  Enable this to automatically run SCHED_ISO for X, this is not default
+	  because there is a slim possibility that other tasks (such as multimedia)
+	  may stutter if they aren't also ran as SCHED_ISO.
+
+	  If unsure, say N here
+
+config BFS_CUSTOM_RR
+	bool "Custom rr_interval for bfs"
+	depends on CPU_BFS
+	default n
+	help
+	  Selecting this option will allow you to change the default value
+	  for rr_interval within the bfs scheduler.
+
+	  The default rr_interval is 6ms
+	  If unsure, say N.
+
+config BFS_RR_INTERVAL
+	int "rr_interval value in ms"
+	depends on BFS_CUSTOM_RR
+	range 1 5000
+	default 6
+	help
+	  This is the smallest duration that any cpu process scheduling unit
+	  will run for. Increasing this value can increase throughput of cpu
+	  bound tasks substantially but at the expense of increased latencies
+	  overall. Conversely decreasing it will decrease average and maximum
+	  latencies but at the expense of throughput. This value is in
+	  milliseconds and the default value chosen depends on the number of
+	  cpus available at scheduler initialisation with a minimum of 6.
+
+	  Valid values are from 1-5000.
+	  Default is 6ms
+	  3ms is recommended for a very interactive, responsive desktop
+
+config CFS_BOOST
+	bool "Boost X Privilege"
+	default y
+	depends on CPU_CFS
+	help
+	  This option instructs the kernel to guarantee more CPU time to
+	  X than to other tasks, which is sueful if you want to have a
+	  faster desktop even under high system load.
+
+	  This option works by automatically boosting X's priority via
+	  renicing it to -10. NOTE: CFS does not suffer from
+	  "overscheduling" problems when X is reniced to -10, so if this
+	  is a predominantly desktop box it makes sense to select this option
+
+	  Say Y here if you are building a kernel for a desktop system.
+	  Say N here if you want X to be treated as a normal task.
+
+config CFS_BOOST_NICE
+	int "Nice value for privileged tasks"
+	depends on CFS_BOOST
+	range -20 19
+	default -10
+
diff --git a/kernel/sched.c b/kernel/sched.c
index 3c11ae0..0e20272 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -6062,6 +6062,47 @@ out_unlock:
 }
 EXPORT_SYMBOL(set_user_nice);
 
+#ifdef CONFIG_CFS_BOOST
+/*
+ * Nice level for privileged tasks. (can be set to 0 for this
+ * to be turned off)
+ */
+int sysctl_sched_privileged_nice_level __read_mostly = CONFIG_CFS_BOOST_NICE;
+
+static int __init privileged_nice_level_setup(char *str)
+{
+	sysctl_sched_privileged_nice_level = simple_strtol(str, NULL, 0);
+	return 1;
+}
+__setup("privileged_nice_level=", privileged_nice_level_setup);
+
+/*
+ * Tasks with special privileges call this and gain extra nice
+ * levels:
+ */
+void sched_privileged_task(struct task_struct *p)
+{
+	long new_nice = sysctl_sched_privileged_nice_level;
+	long old_nice = TASK_NICE(p);
+
+	if (new_nice >= old_nice)
+		return;
+	/*
+	 * Setting the sysctl to 0 turns off the boosting:
+	 */
+	if (unlikely(!new_nice))
+		return;
+
+	if (new_nice < -20)
+		new_nice = -20;
+	else if (new_nice > 19)
+		new_nice = 19;
+
+	set_user_nice(p, new_nice);
+}
+EXPORT_SYMBOL(sched_privileged_task);
+#endif
+
 /*
  * can_nice - check if a task can reduce its nice value
  * @p: task
diff --git a/kernel/sched_bfs.c b/kernel/sched_bfs.c
index ef3230f..a958d6e 100644
--- a/kernel/sched_bfs.c
+++ b/kernel/sched_bfs.c
@@ -116,7 +116,11 @@
  * Value is in ms and set to a minimum of 6ms. Scales with number of cpus.
  * Tunable via /proc interface.
  */
+#ifdef CONFIG_BFS_CUSTOM_RR
+int rr_interval __read_mostly = CONFIG_BFS_RR_INTERVAL;
+#else
 int rr_interval __read_mostly = 6;
+#endif
 
 /*
  * sched_iso_cpu - sysctl which determines the cpu percentage SCHED_ISO tasks
diff --git a/mm/oom_kill.c b/mm/oom_kill.c
index 268dfce..6d53e17 100644
--- a/mm/oom_kill.c
+++ b/mm/oom_kill.c
@@ -365,8 +365,11 @@ static void __oom_kill_task(struct task_struct *p, int verbose)
 	 * all the memory it needs. That way it should be able to
 	 * exit() and clear out its resources quickly...
 	 */
-#ifdef CONFIG_CPU_BFS
+#if defined(CONFIG_CPU_BFS)
 	p->time_slice = HZ;
+#elif defined(CONFIG_CFS_BOOST)
+	if (p->policy == SCHED_NORMAL || p->policy == SCHED_BATCH)
+		sched_privileged_task(p);
 #else
 	p->rt.time_slice = HZ;
 #endif
